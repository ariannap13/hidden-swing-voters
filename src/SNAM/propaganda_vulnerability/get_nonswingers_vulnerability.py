import pandas as pd
import json
import json
import os
import bz2
from concurrent.futures import ThreadPoolExecutor
import threading

# Functions
def load_data(file_path):
    """
    Load data from a JSON file.
    
    Parameters:
        file_path (str): The path to the JSON file.
        
    Returns:
        dict: The loaded data.
    """
    with open(file_path, "r") as f:
        data = json.load(f)
    return data

def process_file(file_path, ids_set, propaganda_set, swing_periods):
    """
    Process twitter file.

    Parameters:
        file_path (str): The path to the JSON file.

        ids_set (list): The list of users IDs to consider.

        propaganda_set (list): The list of tweets IDs from political representatives containing propaganda.

        swing_periods (list): The list of swing periods for the users of interest.

    Returns:
        dict: A dictionary where keys are original tweets (that are retweeted by users), values are tuples with user IDs, swing periods and retweet creation timestamp

    """
    local_dict_overall = {}
    
    # dictionary to map author IDs to their indices
    author_indices = {id_author: index for index, id_author in enumerate(ids_set)}


    with bz2.open(file_path, "rt") as f:
        for line in f:
            try:
                tweet = json.loads(line)

                # time 
                if 'created_at' in tweet:
                    created_at = tweet['created_at']
                else:
                    continue
                
                # author
                if 'author_id' in tweet:
                    author_id = str(tweet['author_id'])
                elif 'user' in tweet:
                    author_id = tweet['user'].get('id_str', "")
                else:
                    continue
                
                # check if the tweet is a retweet
                if author_id not in ids_set:
                    continue

                # Check if the tweet is a retweet
                is_retweet = ("retweeted_status" in tweet and tweet["retweeted_status"]) or \
                             ("referenced_tweets" in tweet and tweet["referenced_tweets"][0]["type"] == "retweeted")
                
                if is_retweet:
                    # extract the retweeted tweet ID
                    retweeted_id = tweet.get("retweeted_status", {}).get("id") or \
                                   (tweet.get("referenced_tweets", [{}])[0].get("id") if "referenced_tweets" in tweet else None)

                    retweeted_id = str(retweeted_id)    

                    # check if the retweeted tweet ID is in the propaganda set
                    if retweeted_id in propaganda_set:
                        # if retweet ID not already in dictionary, initialize an empty list
                        if retweeted_id not in local_dict_overall:
                            local_dict_overall[retweeted_id] = []
                        
                        # get the index of the author_id in the ids_set
                        index = author_indices[author_id]

                        # append the author ID to the list of authors for this retweet ID
                        local_dict_overall[retweeted_id].append((author_id, swing_types[index], created_at))

            except json.JSONDecodeError:
                continue

    return local_dict_overall


def update_progress(future):
    """
    Update the progress of the files processed.

    Parameters:
        future (Future): The future object to get the result from.

    Returns:
        None
    """
    global counter
    dict_overall.update(future.result())
    with lock:
        counter += 1
        print(f"Files processed: {counter}/{total_files}", flush=True)

# file available upon request
swingers_bd = pd.read_csv('../../../data/swing_voters_bd.csv')
swingers_da = pd.read_csv('../../../data/swing_voters_da.csv')
swingers_ba = pd.read_csv('../../../data/swing_voters_ba.csv')

print(f"Number of BD swingers: {len(swingers_bd)}")
print(f"Number of DA swingers: {len(swingers_da)}")
print(f"Number of BA swingers: {len(swingers_ba)}")

# open communities files, available upon request
before_community = pd.read_csv('../../../data/04_communities/comms_with_political_label_csv/pre_campaign_communities.csv')
during_community = pd.read_csv('../../../data/04_communities/comms_with_political_label_csv/during_campaign_communities.csv')
after_community = pd.read_csv('../../../data/04_communities/comms_with_political_label_csv/post_elections_communities.csv')

# subset for users not in swingers_bd, swingers_da, swingers_ba
bd_community = before_community[~before_community['user_id'].isin(swingers_bd['user'])]
ba_community = before_community[~before_community['user_id'].isin(swingers_ba['user'])]
da_community = during_community[~during_community['user_id'].isin(swingers_da['user'])]

# calculate the number of samples to take from each group
samples_per_group_bd = (bd_community.groupby('community_id').size() / bd_community.shape[0] * len(swingers_bd)).round().astype(int)
samples_per_group_da = (da_community.groupby('community_id').size() / da_community.shape[0] * len(swingers_da)).round().astype(int)
samples_per_group_ba = (ba_community.groupby('community_id').size() / ba_community.shape[0] * len(swingers_ba)).round().astype(int)

# stratified sampling
nonswingers_bd = bd_community.groupby('community_id', group_keys=False).apply(lambda x: x.sample(n=samples_per_group_bd.loc[x.name]))
nonswingers_da = da_community.groupby('community_id', group_keys=False).apply(lambda x: x.sample(n=samples_per_group_da.loc[x.name]))
nonswingers_ba = ba_community.groupby('community_id', group_keys=False).apply(lambda x: x.sample(n=samples_per_group_ba.loc[x.name]))

# file available upon request
vips_annotated = load_data('../../../data/06_propaganda/tweets_vips_annotated_cleaned.json')

# initialize a counter and a lock for thread-safe operations
counter = 0
lock = threading.Lock()

# paths to data directories, from Pierri et al. (2023)
data_dirs = [
    "../../../data/ita-2022_twitter_data/search_tweets/",
    "../../../data/ita-2022_twitter_data/tweets/"
]

nonswingers_bd["swing_type"] = "BD"
nonswingers_da["swing_type"] = "DA"
nonswingers_ba["swing_type"] = "BA"

set_nonswingers = pd.concat([nonswingers_bd, nonswingers_da, nonswingers_ba])

set_nonswingers['user_id'] = set_nonswingers['user_id'].astype(str)

ids_set = list(set_nonswingers['user_id'].astype(str).values)
swing_types = list(set_nonswingers['swing_type'].values)
propaganda_set = set(vips_annotated.keys())
print(f"Number of tot. users (not unique): {len(ids_set)}", flush=True)
print(f"Number of unique users: {len(set(ids_set))}", flush=True)

dict_overall = {}
total_files = sum(len(os.listdir(data_dir)) for data_dir in data_dirs)  # Total number of files to process
print(f"Total files to process: {total_files}", flush=True)

with ThreadPoolExecutor() as executor:
    futures = []
    for data_dir in data_dirs:
        files = os.listdir(data_dir)
        for file in files:
            file_path = os.path.join(data_dir, file)
            future = executor.submit(process_file, file_path, ids_set, vips_annotated, swing_types)
            future.add_done_callback(update_progress)
            futures.append(future)

# save dict_overall to a file (json), file available upon request
with open("../../../data/07_propaganda_vulnerability/propaganda_nonswingers.json", "w") as f:
    json.dump(dict_overall, f)
