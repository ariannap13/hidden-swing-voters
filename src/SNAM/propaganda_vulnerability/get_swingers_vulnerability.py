import pandas as pd
import json
import json
import os
import bz2
from concurrent.futures import ThreadPoolExecutor
import threading

# Functions
def load_data(file_path):
    """
    Load data from a JSON file.
    
    Parameters:
        file_path (str): The path to the JSON file.
        
    Returns:
        dict: The loaded data.
    """
    with open(file_path, "r") as f:
        data = json.load(f)
    return data

def process_file(file_path, ids_set, propaganda_set, swing_periods):
    """
    Process twitter file.

    Parameters:
        file_path (str): The path to the JSON file.

        ids_set (list): The list of users IDs to consider.

        propaganda_set (list): The list of tweets IDs from political representatives containing propaganda.

        swing_periods (list): The list of swing periods for the users of interest.

    Returns:
        dict: A dictionary where keys are original tweets (that are retweeted by users), values are tuples with user IDs, swing periods and retweet creation timestamp

    """
    local_dict_overall = {}
    
    # dictionary to map author IDs to their indices
    author_indices = {id_author: index for index, id_author in enumerate(ids_set)}

    with bz2.open(file_path, "rt") as f:
        for line in f:
            try:
                tweet = json.loads(line)

                # time 
                if 'created_at' in tweet:
                    created_at = tweet['created_at']
                else:
                    continue
                
                # author
                if 'author_id' in tweet:
                    author_id = str(tweet['author_id'])
                elif 'user' in tweet:
                    author_id = tweet['user'].get('id_str', "")
                else:
                    continue
                
                # skip tweets from non-identified authors
                if author_id not in ids_set:
                    continue

                # check if the tweet is a retweet
                is_retweet = ("retweeted_status" in tweet and tweet["retweeted_status"]) or \
                             ("referenced_tweets" in tweet and tweet["referenced_tweets"][0]["type"] == "retweeted")
                
                if is_retweet:
                    # extract the retweeted tweet ID
                    retweeted_id = tweet.get("retweeted_status", {}).get("id") or \
                                   (tweet.get("referenced_tweets", [{}])[0].get("id") if "referenced_tweets" in tweet else None)

                    retweeted_id = str(retweeted_id)    

                    # check if the retweeted tweet ID is in the propaganda set
                    if retweeted_id in propaganda_set:
                        # if retweet ID not already in dictionary, initialize an empty list
                        if retweeted_id not in local_dict_overall:
                            local_dict_overall[retweeted_id] = []

                        # get the index of the author_id in the ids_set
                        index = author_indices[author_id]

                        # append the author ID to the list of authors for this retweet ID
                        local_dict_overall[retweeted_id].append((author_id, swing_periods[index], created_at))

            except json.JSONDecodeError:
                continue

    return local_dict_overall


def update_progress(future):
    """
    Update the progress of the files processed.

    Parameters:
        future (Future): The future object to get the result from.

    Returns:
        None
    """
    global counter
    dict_overall.update(future.result())
    with lock:
        counter += 1
        print(f"Files processed: {counter}/{total_files}", flush=True)

# files available upon request
swingers_bd = pd.read_csv('../../../data/Swingers BD.csv')
swingers_da = pd.read_csv('../../../data/Swingers DA.csv')
swingers_ba = pd.read_csv('../../../data/Swingers BA.csv')

print(f"Number of BD swingers: {len(swingers_bd)}")
print(f"Number of DA swingers: {len(swingers_da)}")
print(f"Number of BA swingers: {len(swingers_ba)}")

# file available upon request
vips_annotated = load_data('../../../data/tweets_vips_annotated.json')

# initialize a counter and a lock for thread-safe operations
counter = 0
lock = threading.Lock()

# paths to data directories, from Pierri et al. (2023)
data_dirs = [
   "../../../data/ita-2022_twitter_data/search_tweets/",
    "../../../data/ita-2022_twitter_data/tweets/"
]

swingers_bd["swing_period"] = "BD"
swingers_da["swing_period"] = "DA"
swingers_ba["swing_period"] = "BA"

set_swingers = pd.concat([swingers_bd, swingers_da, swingers_ba])

set_swingers['user'] = set_swingers['user'].astype(str)

ids_set = list(set_swingers['user'].astype(str).values)
swing_periods = list(set_swingers['swing_period'].values)
propaganda_set = set(vips_annotated.keys())
print(f"Number of tot. users (not unique): {len(ids_set)}", flush=True)
print(f"Number of unique users: {len(set(ids_set))}", flush=True)

dict_overall = {}
total_files = sum(len(os.listdir(data_dir)) for data_dir in data_dirs)  # Total number of files to process
print(f"Total files to process: {total_files}", flush=True)

with ThreadPoolExecutor() as executor:
    futures = []
    for data_dir in data_dirs:
        files = os.listdir(data_dir)
        for file in files:
            file_path = os.path.join(data_dir, file)
            future = executor.submit(process_file, file_path, ids_set, vips_annotated, swing_periods)
            future.add_done_callback(update_progress)
            futures.append(future)

# save dict_overall to a file (json), file available upon request
with open("../../../data/propaganda_swingers.json", "w") as f:
    json.dump(dict_overall, f)
